{
  "$comment": "this config is for training on sagemaker without tokenizer",
  "Dataloader": {
    "$comment": "config of Dataloader. data_path is required",
    "data_file": "./data/en-fr_tokenized.parquet"
  },
  "Trainer": {
    "$comment": "config of Trainer. model, num_epoch are required.",
    "type": "TransformerTrainer",
    "num_epoch": 10,
    "num_tokens": 30000,
    "max_num_ckpt": 2,
    "enable_amp": true,
    "dataloader": {
      "type": "get_transformer_dataloader",
      "params": {
        "max_seq_len": 100,
        "batch_size": 32,
        "num_workers": 20,
        "pin_memory": true,
        "train_val_test_split": [0.7, 0.15, 0.15],
        "pad_token_id": 0,
        "bos_token_id": 1,
        "eos_token_id": 2
      }
    },
    "save_period": {
      "value": 100,
      "description": "save checkpoint every 100 batch"
    },
    "resume": {
      "value": true,
      "description": "an example of ckpt_dir: ./checkpoints/transformer/<folder_name>/"
    },
    "model": {
      "type": "Transformer",
      "params": {
        "embed_dim": 512,
        "num_layers": 6,
        "num_heads": 8,
        "feedforward_dim": 2048,
        "pad_token_id": 0
      }
    },
    "optimizer": {
      "type": "AdamW",
      "params": {
        "betas": [0.9, 0.98],
        "eps": 1e-09,
        "weight_decay": 1e-2
      }
    },
    "lr_scheduler": {
      "type": "LambdaLR",
      "lambda": {
        "type": "Transformer_lambda",
        "params": {
          "embed_dim": 512,
          "warmup_steps": 4000
        }
      }
    },
    "criterion": {
      "type": "CrossEntropyLoss",
      "params": {
        "ignore_index": 0,
        "label_smoothing": 0.1
      }
    }
  }
}