{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LLM/NLP Hyperparameters",
  "type": "object",
  "required": ["Dataloader"],
  "properties": {
    "Tokenizer": {
      "type": "object",
      "required": ["type", "load", "src_tokenizer_regex", "tgt_tokenizer_regex", "vocab_size", "all_chars"],
      "properties": {
        "type": {
          "type": "string",
          "enum": ["WordPiece", "BPE"],
          "description": "Word level parser used for tokenizer training and tokenizing"
        },
        "load": {
          "type": "object",
          "required": ["value", "dir"],
          "properties": {
            "value": {
              "type": "boolean"
            },
            "dir": {
              "type": "string",
              "pattern": "^(?:[a-zA-Z]:)?(?:[\\/]?(?:.{1,2}|[^\\/:*?\"<>|\n]+)(?:[\\/](?:.{1,2}|[^\\/:*?\"<>|\n]+))*)?[^\\/:*?\"<>|\n]+"
            }
          }
        },
        "src_tokenizer_regex": {
          "type": "string",
          "format": "regex",
          "description": "Word level parser used for tokenizer training and tokenizing"
        },
        "tgt_tokenizer_regex": {
          "type": "string",
          "format": "regex",
          "description": "Word level parser used for tokenizer training and tokenizing"
        },
        "vocab_size": {
          "type": "integer",
          "minimum": 0,
          "description": "Target vocab size for tokenizer training"
        },
        "all_chars": {
          "default": "",
          "oneOf":[
            {
              "type": "string",
              "description": "String of characters to convert to codepoints"
            },
            {
              "type": "array",
              "items": {
                "type": "integer",
                "minimum": 0
              },
              "description": "Array of Unicode codepoints"
            }
          ],
          "description": "Default tokens to include"
        },
        "special_tokens":{
          "default": [],
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "Default tokens to include"
        },
        "sample_size": {
          "default": 0.1,
          "type": "number",
          "minimum": 0,
          "maximum": 1.0,
          "description": "Fraction of samples to get from dataset for tokenizer training"
        }
      }
    },
    "Dataloader": {
      "type": "object",
      "required": ["data_path"],
      "properties": {
        "data_path": {
          "type": "string",
          "pattern": "^(?:[a-zA-Z]:)?(?:[\\/]?(?:.{1,2}|[^\\/:*?\"<>|\n]+)(?:[\\/](?:.{1,2}|[^\\/:*?\"<>|\n]+))*)?[^\\/:*?\"<>|\n]+.(?:csv|xlsx|json|parquet)$",
          "description": "Word level parser used for tokenizer training and tokenizing"
        },
        "allowed_chars": {
          "default": "",
          "type": ["string", "array"],
          "description": "Characters that are allowed through data clean up"
        },
        "nrows": {
          "default": "None",
          "type": "integer",
          "minimum": 1,
          "description": "Number of rows to read from local file"
        }
      }
    },
    "Trainer":{
      "type": "object",
      "required": ["type", "num_epoch", "dataloader", "model", "optimizer", "criterion"]
    }
  }
}