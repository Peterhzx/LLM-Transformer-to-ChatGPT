{
  "Tokenizer": {
    "$comment": "config of Tokenizer. vocab_size, src_tokenizer_regex, tgt_tokenizer_regex are required",
    "all_chars": [32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,124,160,167,169,8208,8209,8211,8212,8216,8217,8220,8221,8224,8225,8230,8240,8242,8243,8364,171,178,179,187,192,194,198,199,200,201,202,203,206,207,212,217,219,220,224,226,230,231,232,233,234,235,238,239,244,249,251,252,255,338,339,376,691,738,7496,7497,8239,8722],
    "src_tokenizer_regex": "\\d+(?:[\\.,]\\d+)* | \\w+(?:[-']\\w+)* | \\S\\S+ | \\S",
    "tgt_tokenizer_regex": "\\d+(?:[\\.,]\\d+)* | [a-zA-ZàâäéèêëîïôöùûüçÀÂÄÉÈÊËÎÏÔÖÙÛÜÇ]['’] | [a-zA-ZÀÂÆÇÈÉÊËÎÏÔÙÛÜàâæçèéêëîïôùûüÿŒœŸ]+(?:[-'’][a-zA-ZÀÂÆÇÈÉÊËÎÏÔÙÛÜàâæçèéêëîïôùûüÿŒœŸ]+)* | \\S\\S+ | \\S",
    "vocab_size": 30000,
    "special_tokens": ["<PAD>", "<BOS>", "<EOS>"],
    "sample_size": 0.1
  },
  "Dataloader": {
    "$comment": "config of Dataloader. data_path is required",
    "data_path": "D:\\AI\\NLP_Project\\data\\en-fr.csv",
    "allowed_chars": [32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,124,160,167,169,8208,8209,8211,8212,8216,8217,8220,8221,8224,8225,8230,8240,8242,8243,8364,171,178,179,187,192,194,198,199,200,201,202,203,206,207,212,217,219,220,224,226,230,231,232,233,234,235,238,239,244,249,251,252,255,338,339,376,691,738,7496,7497,8239,8722],
    "nrows": 1000000
  }
}